{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion is:  happy\n"
     ]
    }
   ],
   "source": [
    "# # face emotion\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import imutils\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from playsound import playsound\n",
    "\n",
    "# parameters for loading data and images\n",
    "detection_model_path = 'haarcascade_files/haarcascade_frontalface_default.xml'\n",
    "emotion_model_path = 'models/_mini_XCEPTION.102-0.66.hdf5'\n",
    "\n",
    "# hyper-parameters for bounding boxes shape\n",
    "# loading models\n",
    "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "EMOTIONS = [\"angry\" ,\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\n",
    "            \"neutral\"]\n",
    "\n",
    "Face_emotion = ''\n",
    "# starting video streaming\n",
    "cv2.namedWindow('Emotion_classifier')\n",
    "camera = cv2.VideoCapture(0) \n",
    "while True:\n",
    "\n",
    "    emotion_percentage = {\"angry\":0,\"disgust\":0,\"scared\":0, \"happy\":0,\"sad\":0, \"surprised\":0, \"neutral\":0}\n",
    "\n",
    "    frame = camera.read()[1] # this is for live video\n",
    "    #print(frame)\n",
    "    #reading the frame\n",
    "    frame = imutils.resize(frame,width=300)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('img',gray)\n",
    "    faces = face_detection.detectMultiScale(gray,flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    canvas = np.zeros((250, 300, 3), dtype=\"uint8\")\n",
    "    frameClone = frame.copy()\n",
    "\n",
    "    Total_no_of_faces = len(faces)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        faces = sorted(faces, reverse=True,\n",
    "        key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))#[0]\n",
    "        for face in faces:\n",
    "            (fX, fY, fW, fH) = face\n",
    "\n",
    " # Extract the ROI of the face from the grayscale image, resize it to a fixed 28x28 pixels, \n",
    " #and then prepare the ROI for classification via the CNN\n",
    "            \n",
    "            roi = gray[fY:fY + fH, fX:fX + fW]\n",
    "            # print('roi',roi.shape)\n",
    "            roi = cv2.resize(roi, (64, 64))\n",
    "            roi = roi.astype(\"float\") / 255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi, axis=0)\n",
    "            # print('roi',roi.shape)\n",
    "    \n",
    "            preds = emotion_classifier.predict(roi)[0]\n",
    "            #print(preds)\n",
    "            emotion_probability = np.max(preds)\n",
    "            label = EMOTIONS[preds.argmax()]\n",
    "            cv2.putText(frameClone, Face_emotion, (fX, fY - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "            cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),\n",
    "                          (0, 255, 255), 2)\n",
    "            cv2.putText(frameClone,'Press q to capture this emotion', (fX, fY - 25),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        \n",
    "    else: continue\n",
    "\n",
    "    cv2.imshow('Emotion_classifier', frameClone)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Emotion is: \",label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide Voice Signal After  a Beep sound\n"
     ]
    }
   ],
   "source": [
    "#import required modules\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "  \n",
    "# for playing wav file\n",
    "\n",
    "song = AudioSegment.from_wav(\"beep-06.wav\")\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "print('Provide Voice Signal After  a Beep sound')\n",
    "sleep(2)\n",
    "playsound('beep-06.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speak\n",
      "say something!...\n",
      "You said: feel good\n",
      "Message: ['feel good']\n",
      "predicted: neutral (0.34 seconds)\n"
     ]
    }
   ],
   "source": [
    "# # voice emotion\n",
    "\n",
    "print('speak')\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "r = sr.Recognizer()\n",
    "\n",
    "speech = sr.Microphone(device_index=0)\n",
    "\n",
    "with speech as source:\n",
    "    print(\"say something!...\")\n",
    "    audio = r.adjust_for_ambient_noise(source)\n",
    "    audio = r.listen(source)\n",
    "\n",
    "try:\n",
    "    recog = r.recognize_google(audio, language = 'en-US')\n",
    "    print(\"You said: \" + recog)\n",
    "    message = [recog]\n",
    "\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Google Speech Recognition could not understand audio\")\n",
    "except sr.RequestError as e:\n",
    "    print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "\n",
    "\n",
    "# # text \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# text preprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# plots and metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# preparing input to our model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# keras layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "# Number of labels: joy, anger, fear, sadness, neutral\n",
    "num_classes = 5\n",
    "\n",
    "# Number of dimensions for word embedding\n",
    "embed_num_dims = 300\n",
    "\n",
    "# Max input length (max number of words) \n",
    "max_seq_len = 500\n",
    "\n",
    "class_names = ['joy', 'fear', 'anger', 'sadness', 'neutral']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('models/cnn_w2v (copy).h5')\n",
    "\n",
    "# message = [\"feel good\"]\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(message)\n",
    "padded = pad_sequences(seq, maxlen=max_seq_len)\n",
    "\n",
    "start_time = time.time()\n",
    "pred = model.predict(padded)\n",
    "\n",
    "print('Message: ' + str(message))\n",
    "print('predicted: {} ({:.2f} seconds)'.format(class_names[np.argmax(pred)], (time.time() - start_time)))\n",
    "predicted = class_names[np.argmax(pred)]\n",
    "Voice_emotion = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After integration\n",
      "Face emotion is: happy \n",
      "Voice emotion is: happy\n"
     ]
    }
   ],
   "source": [
    "# # for integration \n",
    "# # from face\n",
    "\n",
    "if label == \"angry\":\n",
    "    Face_emotion = \"sad\"\n",
    "\n",
    "if label == \"happy\":\n",
    "    Face_emotion = \"happy\"\n",
    "            \n",
    "if label == \"sad\":\n",
    "    Face_emotion = \"sad\"\n",
    "            \n",
    "if label == \"disgust\":\n",
    "    Face_emotion = \"sad\"\n",
    "            \n",
    "if label == \"scared\":\n",
    "    Face_emotion = \"sad\"\n",
    "            \n",
    "if label == \"surprised\":\n",
    "    Face_emotion = \"happy\"\n",
    "            \n",
    "if label == \"neutral\":\n",
    "    Face_emotion = \"happy\"\n",
    "\n",
    "# # from voice\n",
    "\n",
    "if predicted == 'joy':\n",
    "    Voice_emotion = 'happy'\n",
    "\n",
    "if predicted == 'fear':\n",
    "    Voice_emotion = 'sad'\n",
    "\n",
    "if predicted == 'anger':\n",
    "    Voice_emotion = 'sad'\n",
    "\n",
    "if predicted == 'sadness':\n",
    "    Voice_emotion = 'sad'\n",
    "\n",
    "if predicted == 'neutral':\n",
    "    Voice_emotion = 'happy'   \n",
    "    \n",
    "print(\"After integration\")\n",
    "print(\"Face emotion is:\",Face_emotion,\"\\nVoice emotion is:\",Voice_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy-clappy-ukulele.mp3']\n",
      "OVERALL EMOTION IS: happy\n",
      "press ENTER to stop playbackENTER\n",
      "All OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # checking the predictions and play song accordingly\n",
    "\n",
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "if Face_emotion == Voice_emotion:\n",
    "    #play song from folder\n",
    "    if Face_emotion == 'happy':\n",
    "        \n",
    "        list_of_happy_songs = os.listdir('happy')\n",
    "        song = random.choices(list_of_happy_songs)\n",
    "        print(song)\n",
    "        print(\"OVERALL EMOTION IS: happy\")\n",
    "        p = multiprocessing.Process(target=playsound('happy/'+ song[0]), args=(\"song[0]\",))\n",
    "        input(\"press ENTER to stop playback\")\n",
    "        p.start()\n",
    "        p.terminate()\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        list_of_sad_songs = os.listdir('sad')\n",
    "        song = random.choices(list_of_sad_songs)\n",
    "        print(song)\n",
    "        print(\"OVERALL EMOTION IS: sad\")\n",
    "        \n",
    "        p = multiprocessing.Process(target=playsound('sad/'+ song[0]), args=(\"song[0]\",))\n",
    "        input(\"press ENTER to stop playback\")\n",
    "        p.start()\n",
    "        p.terminate()\n",
    "    print('All OK')\n",
    "else:\n",
    "    print(\"Face Emotion and Voice Emotion Does not Match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
